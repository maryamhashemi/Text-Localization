{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wIb450h0ULF"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Implementation Of Proposed Method in \n",
    "#\"Arbitrary Shape Scene Text Detection with Adaptive Text Region Representation\"\n",
    "\n",
    "**Author: Maryam Sadat Hshemi , Sara Aein**\n",
    "\n",
    "**Download papre : https://arxiv.org/abs/1905.05980**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ZSqJOW2xWuPX",
    "outputId": "3ab01468-12f4-4e6a-9d70-b163a78c48c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BK8yoyp98_qu"
   },
   "source": [
    "<div dir = rtl>\n",
    "شبکه این مقاله دارای دو قسمت است. در قسمت اول یک شبکه text_RPN است که پروپوزال های مکان متن را پیشنهاد می دهد و شبکه دوم یک شبکه LSTM است که این پیشنهادات را بهبود می دهد.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dqj3PbBr1wCt"
   },
   "source": [
    "## 1.Text_RPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9W86TxmK1gNs"
   },
   "source": [
    "### Import Prerequesties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "eFDYbmTRocLt",
    "outputId": "b4966c44-a66f-4fdb-dbb5-1082aba1f8e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPool2D, GlobalAveragePooling2D, multiply\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "codX9q_Z1sOm"
   },
   "source": [
    "### Squeeze and Excitation Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yiKo-uTDGsh5"
   },
   "source": [
    "<div dir = rtl>\n",
    "شبکه اصلی بخش اول، یک شبکه SE_VGG16 است که همان شبکه VGG16 می باشد با این تفاوت که بعد از هر لایه pooling، یک بلوک SE قرار می دهد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fkUFAOS-ocL3"
   },
   "outputs": [],
   "source": [
    "def SE_Block(in_block, name = '', ratio=16):\n",
    "    shape = in_block.shape.as_list()\n",
    "    filters = shape[-1]\n",
    "\n",
    "    x = GlobalAveragePooling2D(name = name + '_GlobalAvgPool')(in_block)\n",
    "    x = Dense(filters // ratio, activation='relu',use_bias= False, name = name + '_1')(x)\n",
    "    x = Dense(filters, activation='sigmoid',use_bias= False, name = name + '_2')(x)\n",
    "    x = multiply([in_block,x],name = name + '_multiply')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmQ3toye2Y_a"
   },
   "source": [
    "### SE_VGG16\n",
    "This is a backbone network of Text_RPN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_0-4l6-ocL-"
   },
   "outputs": [],
   "source": [
    "def SE_VGG16(input_tensor = None):\n",
    "\n",
    "  input_shape = (None, None, 3)\n",
    "\n",
    "  if input_tensor is None:\n",
    "    img_input = Input(shape=input_shape)\n",
    "  else:\n",
    "    if not K.is_keras_tensor(input_tensor):\n",
    "      img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "    else:\n",
    "      img_input = input_tensor\n",
    "\n",
    "  conv1_1 = Conv2D(filters = 64, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv1_1')(img_input)\n",
    "  conv1_2 = Conv2D(filters = 64, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv1_2')(conv1_1)\n",
    "  pool1 = MaxPool2D(pool_size = (2,2), strides = (2,2), name = 'pool1')(conv1_2)\n",
    "  se1 = SE_Block(pool1,'se1')\n",
    "\n",
    "  conv2_1 = Conv2D(filters = 128, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv2_1')(se1)\n",
    "  conv2_2 = Conv2D(filters = 128, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv2_2')(conv2_1)\n",
    "  pool2 = MaxPool2D(pool_size = (2,2), strides = (2,2), name = 'pool2')(conv2_2)\n",
    "  se2 = SE_Block(pool2,'se2')\n",
    "\n",
    "  conv3_1 = Conv2D(filters = 256, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv3_1')(se2)\n",
    "  conv3_2 = Conv2D(filters = 256, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv3_2')(conv3_1)\n",
    "  conv3_3 = Conv2D(filters = 256, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv3_3')(conv3_2)\n",
    "  pool3 = MaxPool2D(pool_size = (2,2), strides = (2,2), name = 'pool3')(conv3_3)\n",
    "  se3 = SE_Block(pool3, 'se3')\n",
    "\n",
    "  conv4_1 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv4_1')(se3)\n",
    "  conv4_2 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv4_2')(conv4_1)\n",
    "  conv4_3 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv4_3')(conv4_2)\n",
    "  pool4 = MaxPool2D(pool_size = (2,2), strides = (2,2), name = 'pool4')(conv4_3)\n",
    "  se4 = SE_Block(pool4, 'se4')\n",
    "\n",
    "  conv5_1 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv5_1')(se4)\n",
    "  conv5_2 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv5_2')(conv5_1)\n",
    "  conv5_3 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv5_3')(conv5_2)\n",
    "  pool5 = MaxPool2D(pool_size = (2,2), strides = (2,2), name = 'pool4')(conv4_3)\n",
    "  se5 = SE_Block(pool5,'se5')\n",
    "\n",
    "  return se5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAwohF6Yl_SM"
   },
   "source": [
    "### Generate Anchor Boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFiwf4qZmIKr"
   },
   "source": [
    "### Region Proposal Network\n",
    "<div dir = rtl>\n",
    "بعد از شبکه اصلی SE_VGG16، یک شبکه RPN باید آموزش داده شود تا پروپوزال های مکان متن را به دست بیاورد.\n",
    "ساختار اRPN در اینجا همانند شبکه RPN موجود در مقاله Faster RCNN است با این تفاوت که اندازه anchor های آن متفاوت است.\n",
    "در این بخش قسمت اصلی شبکه RPN پیاده سازی شده که به عنوان خروجی، محدوده مکانی متن و نوع کلاس هر anchor را می دهد.\n",
    "کلاس در این مسئله منظور کلاس وجود متن و یا نبود آن است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQFkA9Hqmgmk"
   },
   "outputs": [],
   "source": [
    "def rpn_layer(base_layers, num_anchors):\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
    "    \n",
    "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
    "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
    "\n",
    "    return [x_class, x_regr, base_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNFk0XK0MjPS"
   },
   "source": [
    "### ROI Pooling\n",
    "<dir dir=rtl>\n",
    "خروجی شبکه RPN را باید به یک لایه ROI Pooling بدهیم که پرروپوزال های مختلف به دست آمده از شبکه قبل را به یک اندازه مشخص تبدیل کن.\n",
    "این قسمت از سایت زیر گرفته شده‏ است:\n",
    "\n",
    "[faster-rcnn-keras](https://github.com/shadow12138/faster-rcnn-keras)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IHJJeEJMigS"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.engine import Layer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class RoiPoolingConv(Layer):\n",
    "    \"\"\"\n",
    "    ROI pooling layer for 2D inputs.\n",
    "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n",
    "    K. He, X. Zhang, S. Ren, J. Sun\n",
    "    # Arguments\n",
    "        pool_size: int\n",
    "            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n",
    "        num_rois: number of regions of interest to be used\n",
    "    # Input shape\n",
    "        list of two 4D tensors [X_img,X_roi] with shape:\n",
    "        X_img:\n",
    "        `(1, rows, cols, channels)`\n",
    "        X_roi:\n",
    "        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
    "    # Output shape\n",
    "        3D tensor with shape:\n",
    "        `(1, num_rois, channels, pool_size, pool_size)`\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_size, num_rois, **kwargs):\n",
    "        self.dim_ordering = K.image_dim_ordering()\n",
    "        self.pool_size = pool_size\n",
    "        self.num_rois = num_rois\n",
    "\n",
    "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[0][3]\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "    def call(self, x, mask=None):\n",
    "        assert (len(x) == 2)\n",
    "\n",
    "        # x[0] is image with shape (rows, cols, channels)\n",
    "        img = x[0]\n",
    "\n",
    "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
    "        rois = x[1]\n",
    "\n",
    "        input_shape = K.shape(img)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for roi_idx in range(self.num_rois):\n",
    "            x = rois[0, roi_idx, 0]\n",
    "            y = rois[0, roi_idx, 1]\n",
    "            w = rois[0, roi_idx, 2]\n",
    "            h = rois[0, roi_idx, 3]\n",
    "\n",
    "            x = K.cast(x, 'int32')\n",
    "            y = K.cast(y, 'int32')\n",
    "            w = K.cast(w, 'int32')\n",
    "            h = K.cast(h, 'int32')\n",
    "\n",
    "            # Resized roi of the image to pooling size (7x7)\n",
    "            rs = tf.image.resize_images(img[:, y:y + h, x:x + w, :], (self.pool_size, self.pool_size))\n",
    "            outputs.append(rs)\n",
    "\n",
    "        final_output = K.concatenate(outputs, axis=0)\n",
    "\n",
    "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
    "        # Might be (1, 4, 7, 7, 3)\n",
    "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
    "\n",
    "        # permute_dimensions is similar to transpose\n",
    "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "\n",
    "        return final_output\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {'pool_size': self.pool_size,\n",
    "                  'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingConv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def get_img_output_length(width, height):\n",
    "    def get_output_length(input_length):\n",
    "        return input_length // 16\n",
    "\n",
    "    return get_output_length(width), get_output_length(height)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opDtnQqSmlj_"
   },
   "source": [
    "## 2.Text Refinement Network\n",
    "<div dir=rtl>\n",
    "در این قسمت چون میدانیم تعداد جفت نقاط هر متن نامشخص است، از یک شبکه RNN باید استفاده کنیم که در مقاله از شبکه LSTM استفاده شده است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNbTSU-Jmqhf"
   },
   "outputs": [],
   "source": [
    "def LSTM(roi_input):\n",
    "  return keras.layers.LSTM(roi_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkEvV7OPL9nv"
   },
   "source": [
    "## 3.Setting Config\n",
    "<div dir=rtl>\n",
    "در این بخش اطلاعات کلی شبکه اعم از نوع شبکه اصلی، تعداد anchor ها و مقیاس های آن ها، اندازه تصویر ورودی، میزان گام در RPN، تعداد و برچسب کلاس ها و ... پیکربندی می شود.\n",
    "این قسمت از سایت زیر گرفته شده‏ است:\n",
    "\n",
    "[faster-rcnn-keras](https://github.com/shadow12138/faster-rcnn-keras)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6s94YkOTMGTq"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\n",
    "\t\t# Name of base network\n",
    "\t\tself.network = 'se_vgg16'\n",
    "\n",
    "\t\t# Anchor box scales\n",
    "    # Note that if im_size is smaller, anchor_box_scales should be scaled\n",
    "    # Original anchor_box_scales in the paper is [32,64, 128, 256, 512]\n",
    "\t\tself.anchor_box_scales = [32, 64, 128, 256, 512] \n",
    "\n",
    "\t\t# Anchor box ratios\n",
    "\t\tself.anchor_box_ratios = [[0.5, 0.5], [1, 1], [2, 2]]\n",
    "\n",
    "\t\t# Size to resize the smallest side of the image\n",
    "\t\t# Original setting in paper is 600.\n",
    "\t\tself.im_size = 300\t\t\n",
    "\n",
    "\t\t# image channel-wise mean to subtract\n",
    "\t\tself.img_channel_mean = [103.939, 116.779, 123.68]\n",
    "\t\tself.img_scaling_factor = 1.0\n",
    "\n",
    "\t\t# number of ROIs at once\n",
    "\t\tself.num_rois = 4\n",
    "\n",
    "\t\t# stride at the RPN (this depends on the network configuration)\n",
    "\t\tself.rpn_stride = 16\n",
    "\n",
    "\t\tself.balanced_classes = False\n",
    "\n",
    "\t\t# scaling the stdev\n",
    "\t\tself.std_scaling = 4.0\n",
    "\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "\n",
    "\t\t# overlaps for RPN\n",
    "\t\tself.rpn_min_overlap = 0.3\n",
    "\t\tself.rpn_max_overlap = 0.7\n",
    "\n",
    "\t\t# overlaps for classifier ROIs\n",
    "\t\tself.classifier_min_overlap = 0.1\n",
    "\t\tself.classifier_max_overlap = 0.5\n",
    "\n",
    "\t\t# placeholder for the class mapping, automatically generated by the parser\n",
    "\t\tself.class_mapping = {'text': 1, 'bg': 0}\n",
    "\n",
    "\t\tself.model_path = None\n",
    "\t\tself.training_annotation = None\n",
    "\t\tself.cfg_save_path = None\n",
    "\t\tself.classes_count = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1flFYxMbIHD8"
   },
   "source": [
    "## 4.Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BAZSe8WTIYKN"
   },
   "source": [
    "### Data Processing\n",
    "<div dir=rtl>\n",
    "در این قسمت یک سری عمیلیات کلی که قرار است روی داده ها انجام بگیرد تعریف شده است. اعم از گرفتن داده ها و قرار دادن آن ها به ترتیب مشخص در لیست، تغییر اندازه تصاویر و ... .\n",
    "این قسمت از سایت زیر گرفته شده‏ است:\n",
    "\n",
    "[faster-rcnn-keras](https://github.com/shadow12138/faster-rcnn-keras)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1wuo2220IXW9"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def get_new_img_size(width, height, img_min_side=300):\n",
    "    if width <= height:\n",
    "        f = float(img_min_side) / width\n",
    "        resized_height = int(f * height)\n",
    "        resized_width = img_min_side\n",
    "    else:\n",
    "        f = float(img_min_side) / height\n",
    "        resized_width = int(f * width)\n",
    "        resized_height = img_min_side\n",
    "\n",
    "    return resized_width, resized_height\n",
    "\n",
    "def GetData(annotation_file_path, class_mapping):\n",
    "    \"\"\"\n",
    "    annotation_file should be like the following:\n",
    "    image_path x1,y1,x2,y2,cls1_id x1,y1,x2,y2,cls2_id\n",
    "    for example: pic1.png 0,0,100,100,0 200,200,350,300,1\n",
    "    :param annotation_file_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    classes_count = {}\n",
    "    class_mapping2 = {}\n",
    "    for key in class_mapping:\n",
    "        classes_count[key] = 0\n",
    "        class_mapping2[class_mapping[key]] = key\n",
    "\n",
    "    all_data = []\n",
    "    file = open(annotation_file_path, 'r')\n",
    "    for line in file.readlines():\n",
    "        line = line.strip().split()\n",
    "        bboxes = []\n",
    "        for bbox in line[2:]:\n",
    "            x1, y1, x2, y2, cls_id = map(int, bbox.split(','))\n",
    "            cls = class_mapping2[cls_id]\n",
    "            bboxes.append({'class': cls, 'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n",
    "            classes_count[cls] += 1\n",
    "\n",
    "        all_data.append({\n",
    "            'filepath': line[0],\n",
    "            'height': int(line[1][0]),\n",
    "            'width': int(line[1][1]),\n",
    "            'bboxes': bboxes\n",
    "        })\n",
    "    file.close()\n",
    "    return all_data, classes_count, class_mapping\n",
    "\n",
    "def create_dir(root, delete=False):\n",
    "    if os.path.exists(root):\n",
    "        if not delete:\n",
    "            return\n",
    "        shutil.rmtree(root)\n",
    "    os.makedirs(root)\n",
    "\n",
    "def format_img_size(img, C):\n",
    "    \"\"\" formats the image size based on config \"\"\"\n",
    "    img_min_side = float(C.im_size)\n",
    "    (height, width, _) = img.shape\n",
    "\n",
    "    if width <= height:\n",
    "        ratio = img_min_side / width\n",
    "        new_height = int(ratio * height)\n",
    "        new_width = int(img_min_side)\n",
    "    else:\n",
    "        ratio = img_min_side / height\n",
    "        new_width = int(ratio * width)\n",
    "        new_height = int(img_min_side)\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    return img, ratio\n",
    "\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "    real_x1 = int(round(x1 // ratio))\n",
    "    real_y1 = int(round(y1 // ratio))\n",
    "    real_x2 = int(round(x2 // ratio))\n",
    "    real_y2 = int(round(y2 // ratio))\n",
    "    return real_x1, real_y1, real_x2, real_y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pwS2Q-aqx4y"
   },
   "source": [
    "### Make annotation file.\n",
    "<div dir= rtl>\n",
    "در این قسمت دادگان ICDAR2015 گرفته شده و طبق فرمت خاصی، annotation هی آن ها در فایل هایی ذخیره می شود تا در ادامه قابل استفاده باشد.\n",
    "در این مجموعه دادگان، مکان هر کلمه با چهارنقطه چهار گوشه آن برچسب گذاری شده اند که ما فقط نقاط گوشه چپ و بالا و رات و پایین را به عنوان annotation آن استفاده می کنیم. در انتهای ثبت شدن دو نقطه گوشه متن نیز کلاس آن را که وجود متن است، با 1 مشخص کرده ایم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihJR17u7yUWN"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "annotation_file_path = '/content/drive/My Drive/ICDAR2015/annotation_file.txt'\n",
    "annotation_file = open(annotation_file_path,\"w+\")\n",
    "imagePathes = '/content/drive/My Drive/ICDAR2015/images/'\n",
    "gtPathes = '/content/drive/My Drive/ICDAR2015/gt/'\n",
    "\n",
    "files = glob.glob(imagePathes + '*.png')\n",
    "for imgPath in files:\n",
    "    imgName = imgPath[:-4]\n",
    "    gtPath =  'gt_'+imgName+'.txt'\n",
    "    gtFile = open(gtPathes+gtPath,\"r\")\n",
    "\n",
    "    annotationLine = imgPath\n",
    "\n",
    "    i = 0\n",
    "    for line in gtFile.readlines():\n",
    "        line = line.strip().split()\n",
    "        bboxes = []\n",
    "        for bbox in line:\n",
    "            if i == 0:\n",
    "                \n",
    "                i = 1\n",
    "              \n",
    "            x1, y1 = map(int, bbox.split(','))[0:1]\n",
    "            x2, y2 = map(int, bbox.split(','))[4:5]\n",
    "            cls_id = 1\n",
    "\n",
    "            annotationLine += ' '+x1+','+y1+','+x2+','+y2+','+cls_id\n",
    "    annotation_file.write(annotationLine)\n",
    "annotation_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SioHa2zmSjn3"
   },
   "source": [
    "<div dir=rtl>\n",
    "در این  قسمت تابع GetData فراخوانی شده و به عنوان خروجی دادگان آموزشی و تعداد داده های هر کلاس و نوع نگاشتی که برای کلاس ها در نظر گرفته شده را می دهد.\n",
    "و سپس این اطلاعات را به عنوان اطلاعات پیکربندی ذخیره می کند.\n",
    "این قسمت از سایت زیر گرفته شده‏ است:\n",
    "\n",
    "[faster-rcnn-keras](https://github.com/shadow12138/faster-rcnn-keras)\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcCCJHUMLc9t"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "    training_images, classes_count, class_mapping = GetData(C.training_annotation, C.class_mapping)\n",
    "    C.class_mapping = class_mapping\n",
    "    C.classes_count = classes_count\n",
    "\n",
    "    # Shuffle the images with seed\n",
    "    random.seed(1)\n",
    "    random.shuffle(training_images)\n",
    "\n",
    "    # Get train data generator which generate X, Y, image_data\n",
    "    data_gen_train = anchor.get_anchor_gt(training_images, cfg, net.get_img_output_length, mode='train')\n",
    "    return data_gen_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zJAMiXOLZed"
   },
   "source": [
    "## 5.Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJylPxe5JpBP"
   },
   "source": [
    "### Anchor\n",
    "<div dir=rtl>\n",
    "در این بخش ground truth هر anchor به عنوان برچسب مشخص می شود.\n",
    "این قسمت از سایت زیر گرفته شده‏ است:\n",
    "\n",
    "[faster-rcnn-keras](https://github.com/shadow12138/faster-rcnn-keras)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MbdXyU-2Joe4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def get_anchor_gt(all_img_data, C, img_length_calc_function, mode='train'):\n",
    "    \"\"\" Yield the ground-truth anchors as Y (labels)\n",
    "    Args:\n",
    "        all_img_data: list(filepath, width, height, list(bboxes))\n",
    "        C: config\n",
    "        img_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size\n",
    "        mode: 'train' or 'test'; 'train' mode need augmentation\n",
    "    Returns:\n",
    "        x_img: image data after resized and scaling (smallest size = 300px)\n",
    "        Y: [y_rpn_cls, y_rpn_regr]\n",
    "        img_data_aug: augmented image data (original image with augmentation)\n",
    "        debug_img: show image for debug\n",
    "        num_pos: show number of positive anchors for debug\n",
    "    \"\"\"\n",
    "    while True:\n",
    "\n",
    "        for img_data in all_img_data:\n",
    "            try:\n",
    "\n",
    "                # read in image, and optionally add augmentation\n",
    "\n",
    "                if mode == 'train':\n",
    "                    img_data_aug, x_img = augment(img_data, C, augment=True)\n",
    "                else:\n",
    "                    img_data_aug, x_img = augment(img_data, C, augment=False)\n",
    "\n",
    "                (width, height) = (img_data_aug['width'], img_data_aug['height'])\n",
    "                (rows, cols, _) = x_img.shape\n",
    "\n",
    "                assert cols == width\n",
    "                assert rows == height\n",
    "\n",
    "                # get image dimensions for resizing\n",
    "                (resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n",
    "\n",
    "                # resize the image so that smalles side is length = 300px\n",
    "                x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n",
    "                debug_img = x_img.copy()\n",
    "\n",
    "                try:\n",
    "                    y_rpn_cls, y_rpn_regr, num_pos = calc_rpn(C, img_data_aug, width, height, resized_width,\n",
    "                                                              resized_height, img_length_calc_function)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # Zero-center by mean pixel, and preprocess image\n",
    "\n",
    "                x_img = x_img[:, :, (2, 1, 0)]  # BGR -> RGB\n",
    "                x_img = x_img.astype(np.float32)\n",
    "                x_img[:, :, 0] -= C.img_channel_mean[0]\n",
    "                x_img[:, :, 1] -= C.img_channel_mean[1]\n",
    "                x_img[:, :, 2] -= C.img_channel_mean[2]\n",
    "                x_img /= C.img_scaling_factor\n",
    "\n",
    "                x_img = np.transpose(x_img, (2, 0, 1))\n",
    "                x_img = np.expand_dims(x_img, axis=0)\n",
    "\n",
    "                y_rpn_regr[:, y_rpn_regr.shape[1] // 2:, :, :] *= C.std_scaling\n",
    "\n",
    "                x_img = np.transpose(x_img, (0, 2, 3, 1))\n",
    "                y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))\n",
    "                y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))\n",
    "\n",
    "                yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug, debug_img, num_pos\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39Vxd_S-KJr-"
   },
   "source": [
    "### IOU\n",
    "<div dir=rtl>\n",
    "\n",
    "برای وقتی که بخواهیم کلاس یک ROI را مشخص کنیم، نیاز است که محدوده آن را با groun truth که به عنوان برچسب داریم مقایسه کنیم. اگر میزان IOU آن ها از حدی بیشتر بود به صورت احتمالاتی به عنوان تشخیص درست درنظر گرفته می شوند.\n",
    "در اینجا تابع IOU به این منظور پیاده سازی شده است.\n",
    "این قسمت از سایت زیر گرفته شده‏ است:\n",
    "\n",
    "[faster-rcnn-keras](https://github.com/shadow12138/faster-rcnn-keras)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzLyLNwEKI0r"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def union(au, bu, area_intersection):\n",
    "    area_a = (au[2] - au[0]) * (au[3] - au[1])\n",
    "    area_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n",
    "    area_union = area_a + area_b - area_intersection\n",
    "    return area_union\n",
    "\n",
    "def intersection(ai, bi):\n",
    "    x = max(ai[0], bi[0])\n",
    "    y = max(ai[1], bi[1])\n",
    "    w = min(ai[2], bi[2]) - x\n",
    "    h = min(ai[3], bi[3]) - y\n",
    "    if w < 0 or h < 0:\n",
    "        return 0\n",
    "    return w * h\n",
    "\n",
    "def iou(a, b):\n",
    "    # a and b should be (x1,y1,x2,y2)\n",
    "\n",
    "    if a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n",
    "        return 0.0\n",
    "\n",
    "    area_i = intersection(a, b)\n",
    "    area_u = union(a, b, area_i)\n",
    "\n",
    "    return float(area_i) / float(area_u + 1e-6)\n",
    "\n",
    "def calc_iou(R, img_data, C, class_mapping):\n",
    "    \"\"\"Converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "    Args:\n",
    "        R: bboxes, probs\n",
    "    \"\"\"\n",
    "    bboxes = img_data['bboxes']\n",
    "    (width, height) = (img_data['width'], img_data['height'])\n",
    "    # get image dimensions for resizing\n",
    "    (resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n",
    "\n",
    "    gta = np.zeros((len(bboxes), 4))\n",
    "\n",
    "    for bbox_num, bbox in enumerate(bboxes):\n",
    "        # get the GT box coordinates, and resize to account for image resizing\n",
    "        # gta[bbox_num, 0] = (40 * (600 / 800)) / 16 = int(round(1.875)) = 2 (x in feature map)\n",
    "        gta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width)) / C.rpn_stride))\n",
    "        gta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width)) / C.rpn_stride))\n",
    "        gta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height)) / C.rpn_stride))\n",
    "        gta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height)) / C.rpn_stride))\n",
    "\n",
    "    x_roi = []\n",
    "    y_class_num = []\n",
    "    y_class_regr_coords = []\n",
    "    y_class_regr_label = []\n",
    "    IoUs = []  # for debugging only\n",
    "\n",
    "    # R.shape[0]: number of bboxes (=300 from non_max_suppression)\n",
    "    for ix in range(R.shape[0]):\n",
    "        (x1, y1, x2, y2) = R[ix, :]\n",
    "        x1 = int(round(x1))\n",
    "        y1 = int(round(y1))\n",
    "        x2 = int(round(x2))\n",
    "        y2 = int(round(y2))\n",
    "\n",
    "        best_iou = 0.0\n",
    "        best_bbox = -1\n",
    "        # Iterate through all the ground-truth bboxes to calculate the iou\n",
    "        for bbox_num in range(len(bboxes)):\n",
    "            curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])\n",
    "\n",
    "            # Find out the corresponding ground-truth bbox_num with larget iou\n",
    "            if curr_iou > best_iou:\n",
    "                best_iou = curr_iou\n",
    "                best_bbox = bbox_num\n",
    "\n",
    "        if best_iou < C.classifier_min_overlap:\n",
    "            continue\n",
    "        else:\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            x_roi.append([x1, y1, w, h])\n",
    "            IoUs.append(best_iou)\n",
    "\n",
    "            if C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:\n",
    "                # hard negative example\n",
    "                cls_name = 'bg'\n",
    "            elif C.classifier_max_overlap <= best_iou:\n",
    "                cls_name = bboxes[best_bbox]['class']\n",
    "                cxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n",
    "                cyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n",
    "\n",
    "                cx = x1 + w / 2.0\n",
    "                cy = y1 + h / 2.0\n",
    "\n",
    "                tx = (cxg - cx) / float(w)\n",
    "                ty = (cyg - cy) / float(h)\n",
    "                tw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n",
    "                th = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n",
    "            else:\n",
    "                print('roi = {}'.format(best_iou))\n",
    "                raise RuntimeError\n",
    "\n",
    "        class_num = class_mapping[cls_name]\n",
    "        class_label = len(class_mapping) * [0]\n",
    "        class_label[class_num] = 1\n",
    "        y_class_num.append(copy.deepcopy(class_label))\n",
    "        coords = [0] * 4 * (len(class_mapping) - 1)\n",
    "        labels = [0] * 4 * (len(class_mapping) - 1)\n",
    "        if cls_name != 'bg':\n",
    "            label_pos = 4 * class_num\n",
    "            sx, sy, sw, sh = C.classifier_regr_std\n",
    "            coords[label_pos:4 + label_pos] = [sx * tx, sy * ty, sw * tw, sh * th]\n",
    "            labels[label_pos:4 + label_pos] = [1, 1, 1, 1]\n",
    "            y_class_regr_coords.append(copy.deepcopy(coords))\n",
    "            y_class_regr_label.append(copy.deepcopy(labels))\n",
    "        else:\n",
    "            y_class_regr_coords.append(copy.deepcopy(coords))\n",
    "            y_class_regr_label.append(copy.deepcopy(labels))\n",
    "\n",
    "    if len(x_roi) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n",
    "    X = np.array(x_roi)\n",
    "    # one hot code for bboxes from above => x_roi (X)\n",
    "    Y1 = np.array(y_class_num)\n",
    "    # corresponding labels and corresponding gt bboxes\n",
    "    Y2 = np.concatenate([np.array(y_class_regr_label), np.array(y_class_regr_coords)], axis=1)\n",
    "\n",
    "    return np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_NiwVLQMPsY"
   },
   "source": [
    "### Loss\n",
    "<div dir=rtl>\n",
    "این قسمت از سایت زیر گرفته شده‏ است:\n",
    "\n",
    "[faster-rcnn-keras](https://github.com/shadow12138/faster-rcnn-keras)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uU034HB4MSah"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.objectives import categorical_crossentropy\n",
    "\n",
    "lambda_rpn_regr = 1.0\n",
    "lambda_rpn_class = 1.0\n",
    "\n",
    "lambda_cls_regr = 1.0\n",
    "lambda_cls_class = 1.0\n",
    "\n",
    "epsilon = 1e-4\n",
    "\n",
    "# train.py\n",
    "def rpn_loss_regr(num_anchors):\n",
    "    \"\"\"Loss function for rpn regression\n",
    "    Args:\n",
    "        num_anchors: number of anchors (9 in here)\n",
    "    Returns:\n",
    "        Smooth L1 loss function\n",
    "                           0.5*x*x (if x_abs < 1)\n",
    "                           x_abx - 0.5 (otherwise)\n",
    "    \"\"\"\n",
    "\n",
    "    def rpn_loss_regr_fixed_num(y_true, y_pred):\n",
    "        # x is the difference between true value and predicted vaue\n",
    "        x = y_true[:, :, :, 4 * num_anchors:] - y_pred\n",
    "\n",
    "        # absolute value of x\n",
    "        x_abs = K.abs(x)\n",
    "\n",
    "        # If x_abs <= 1.0, x_bool = 1\n",
    "        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n",
    "\n",
    "        return lambda_rpn_regr * K.sum(\n",
    "            y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(\n",
    "            epsilon + y_true[:, :, :, :4 * num_anchors])\n",
    "\n",
    "    return rpn_loss_regr_fixed_num\n",
    "\n",
    "# train.py\n",
    "def rpn_loss_cls(num_anchors):\n",
    "    \"\"\"Loss function for rpn classification\n",
    "    Args:\n",
    "        num_anchors: number of anchors (9 in here)\n",
    "        y_true[:, :, :, :9]: [0,1,0,0,0,0,0,1,0] means only the second and the eighth box is valid which contains pos or neg anchor => isValid\n",
    "        y_true[:, :, :, 9:]: [0,1,0,0,0,0,0,0,0] means the second box is pos and eighth box is negative\n",
    "    Returns:\n",
    "        lambda * sum((binary_crossentropy(isValid*y_pred,y_true))) / N\n",
    "    \"\"\"\n",
    "\n",
    "    def rpn_loss_cls_fixed_num(y_true, y_pred):\n",
    "        return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :],\n",
    "                                                                                              y_true[:, :, :,\n",
    "                                                                                              num_anchors:])) / K.sum(\n",
    "            epsilon + y_true[:, :, :, :num_anchors])\n",
    "\n",
    "    return rpn_loss_cls_fixed_num\n",
    "\n",
    "# train.py\n",
    "def class_loss_regr(num_classes):\n",
    "    \"\"\"Loss function for rpn regression\n",
    "    Args:\n",
    "        num_anchors: number of anchors (9 in here)\n",
    "    Returns:\n",
    "        Smooth L1 loss function\n",
    "                           0.5*x*x (if x_abs < 1)\n",
    "                           x_abx - 0.5 (otherwise)\n",
    "    \"\"\"\n",
    "\n",
    "    def class_loss_regr_fixed_num(y_true, y_pred):\n",
    "        x = y_true[:, :, 4 * num_classes:] - y_pred\n",
    "        x_abs = K.abs(x)\n",
    "        x_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n",
    "        return lambda_cls_regr * K.sum(\n",
    "            y_true[:, :, :4 * num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(\n",
    "            epsilon + y_true[:, :, :4 * num_classes])\n",
    "\n",
    "    return class_loss_regr_fixed_num\n",
    "\n",
    "\n",
    "def class_loss_cls(y_true, y_pred):\n",
    "    return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzRu900SXCvk"
   },
   "source": [
    "<div dir=rtl>\n",
    "در این بخش مراحل آموزش پیاده سازی شده است. که ایتدا مدل شبکه اصلی ساخته شده و سپس RPN و در نهایت شبکه LSTM. و سپس هر کدام به صورت جدا و همچنین ترکیبی compile شده اند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tB9kpUUJZHv"
   },
   "outputs": [],
   "source": [
    "data_gen_train = get_data()\n",
    "input_shape_img = (None, None, 3)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))\n",
    "\n",
    "# define the base network\n",
    "shared_layers = SE_VGG16(img_input)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios) # 15\n",
    "rpn = rpn_layer(shared_layers, num_anchors)\n",
    "\n",
    "rn = LSTM(roi_input)\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_rn = Model([img_input, roi_input], rn)\n",
    "\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + rn)\n",
    "\n",
    "optimizer = Adam(lr=0.001)\n",
    "optimizer_rn = Adam(lr=1e-5)\n",
    "\n",
    "model_rpn.compile(optimizer=optimizer, loss=[rpn_loss_cls(num_anchors), rpn_loss_regr(num_anchors)])\n",
    "# model_rn.compile(optimizer=optimizer_rn, loss=,metrics={'dense_class_{}'.format(len(cfg.classes_count)): 'accuracy'})\n",
    "# model_all.compile(optimizer='sgd', loss='mae')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Arbitrary Shape Scene Text Detection with Adaptive Text Region Representation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
