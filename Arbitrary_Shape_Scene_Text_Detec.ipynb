{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Arbitrary_Shape_Scene_Text_Detec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wIb450h0ULF",
        "colab_type": "text"
      },
      "source": [
        "# Implementation Of Proposed Method in \n",
        "#\"Arbitrary Shape Scene Text Detection with Adaptive Text Region Representation\"\n",
        "\n",
        "**Author: Maryam Sadat Hshemi , Sara Aein**\n",
        "\n",
        "**Download papre : https://arxiv.org/abs/1905.05980**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqj3PbBr1wCt",
        "colab_type": "text"
      },
      "source": [
        "## 1.Text_RPN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W86TxmK1gNs",
        "colab_type": "text"
      },
      "source": [
        "### Import Prerequesties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFDYbmTRocLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPool2D, GlobalAveragePooling2D, multiply"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "codX9q_Z1sOm",
        "colab_type": "text"
      },
      "source": [
        "### Squeeze and Excitation Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkUFAOS-ocL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SE_Block(in_block, name = '', ratio=16):\n",
        "    shape = in_block.shape.as_list()\n",
        "    filters = shape[-1]\n",
        "\n",
        "    x = GlobalAveragePooling2D(name = name + '_GlobalAvgPool')(in_block)\n",
        "    x = Dense(filters // ratio, activation='relu',use_bias= False, name = name + '_1')(x)\n",
        "    x = Dense(filters, activation='sigmoid',use_bias= False, name = name + '_2')(x)\n",
        "    x = multiply([in_block,x],name = name + '_multiply')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmQ3toye2Y_a",
        "colab_type": "text"
      },
      "source": [
        "### SE_VGG16\n",
        "This is a backbone network of Text_RPN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_0-4l6-ocL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SE_VGG16(img_shape):\n",
        "  input = Input(img_shape, name = 'input')\n",
        "  conv1_1 = Conv2D(filters = 64, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv1_1')(input)\n",
        "  conv1_2 = Conv2D(filters = 64, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv1_2')(conv1_1)\n",
        "  pool1 = MaxPool2D(pool_size = (2,2), strides = (2,2), name = 'pool1')(conv1_2)\n",
        "  se1 = SE_Block(pool1,'se1')\n",
        "\n",
        "  conv2_1 = Conv2D(filters = 128, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv2_1')(se1)\n",
        "  conv2_2 = Conv2D(filters = 128, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv2_2')(conv2_1)\n",
        "  pool2 = MaxPool2D(pool_size = (2,2), strides = (2,2), name = 'pool2')(conv2_2)\n",
        "  se2 = SE_Block(pool2,'se2')\n",
        "\n",
        "  conv3_1 = Conv2D(filters = 256, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv3_1')(se2)\n",
        "  conv3_2 = Conv2D(filters = 256, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv3_2')(conv3_1)\n",
        "  conv3_3 = Conv2D(filters = 256, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv3_3')(conv3_2)\n",
        "  pool3 = MaxPool2D(pool_size = (2,2), strides = (2,2), name = 'pool3')(conv3_3)\n",
        "  se3 = SE_Block(pool3, 'se3')\n",
        "\n",
        "  conv4_1 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv4_1')(se3)\n",
        "  conv4_2 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv4_2')(conv4_1)\n",
        "  conv4_3 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv4_3')(conv4_2)\n",
        "  pool4 = MaxPool2D(pool_size = (2,2), strides = (2,2), name = 'pool4')(conv4_3)\n",
        "  se4 = SE_Block(pool4, 'se4')\n",
        "\n",
        "  conv5_1 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv5_1')(se4)\n",
        "  conv5_2 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv5_2')(conv5_1)\n",
        "  conv5_3 = Conv2D(filters = 512, kernel_size =(3,3), padding = 'same', activation ='relu', name = 'conv5_3')(conv5_2)\n",
        "  pool5 = MaxPool2D(pool_size = (2,2), strides = (2,2), name = 'pool4')(conv4_3)\n",
        "  se5 = SE_Block(pool5,'se5')\n",
        "\n",
        "  model = Model(inputs=[input], outputs=[se5]) \n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAwohF6Yl_SM",
        "colab_type": "text"
      },
      "source": [
        "### Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THisYk2WocMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFiwf4qZmIKr",
        "colab_type": "text"
      },
      "source": [
        "### Region Proposal Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLKFqbV1KMjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RPN(train_path, num_anchor_box_scales, num_anchor_box_ratios):\n",
        "    # -*- coding: utf-8 -*-\n",
        "    # stuff\n",
        "    from __future__ import division\n",
        "    import random\n",
        "    import pprint\n",
        "    import keras\n",
        "    import sys\n",
        "    import time\n",
        "    import numpy as np\n",
        "    from optparse import OptionParser\n",
        "    import pickle\n",
        "    import os\n",
        "\n",
        "    from keras import backend as K\n",
        "    from keras.optimizers import Adam, SGD, RMSprop\n",
        "    from keras.layers import Input\n",
        "    from keras.models import Model\n",
        "    from keras_frcnn import data_generators\n",
        "    from keras_frcnn import config\n",
        "    from keras_frcnn import losses as losses\n",
        "    import keras_frcnn.roi_helpers as roi_helpers\n",
        "    from keras.utils import generic_utils\n",
        "\n",
        "    # # gpu setting\n",
        "    # if 'tensorflow' == K.backend():\n",
        "    #     import tensorflow as tf\n",
        "    # from keras.backend.tensorflow_backend import set_session\n",
        "    # config2 = tf.ConfigProto()\n",
        "    # config2.gpu_options.allow_growth = True\n",
        "    # set_session(tf.Session(config=config2))\n",
        "\n",
        "    # make dirs to save rpn\n",
        "    # \"./models/rpn/rpn\"\n",
        "    if not os.path.isdir(\"models\"):\n",
        "      os.mkdir(\"models\")\n",
        "    if not os.path.isdir(\"models/rpn\"):\n",
        "      os.mkdir(\"models/rpn\")\n",
        "\n",
        "\n",
        "    from keras_frcnn.pascal_voc_parser import get_data\n",
        "\n",
        "    # pass the settings from the command line, and persist them in the config object\n",
        "    C = config.Config()\n",
        "\n",
        "    # set data argumentation\n",
        "    C.use_horizontal_flips = False\n",
        "    C.use_vertical_flips = False\n",
        "    C.rot_90 = False\n",
        "\n",
        "    C.model_path = './model_frcnn.hdf5'\n",
        "    C.num_rois = 10\n",
        "\n",
        "    C.network = 'vgg16'\n",
        "    from keras_frcnn import vgg as nn\n",
        "\n",
        "    C.base_net_weights = nn.get_weight_path()\n",
        "\n",
        "\n",
        "    # place weight files on your directory\n",
        "    base_net_weights = nn.get_weight_path()\n",
        "\n",
        "\n",
        "    #### load images here ####\n",
        "    # get voc images\n",
        "\n",
        "    # '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
        "    all_imgs, classes_count, class_mapping = get_data(train_path)\n",
        "\n",
        "    print(classes_count)\n",
        "\n",
        "    # add background class as 21st class\n",
        "    if 'bg' not in classes_count:\n",
        "      classes_count['bg'] = 0\n",
        "      class_mapping['bg'] = len(class_mapping)\n",
        "\n",
        "    C.class_mapping = class_mapping\n",
        "\n",
        "    inv_map = {v: k for k, v in class_mapping.items()}\n",
        "\n",
        "    print('Training images per class:')\n",
        "    pprint.pprint(classes_count)\n",
        "    print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
        "\n",
        "    config_output_filename = \"config.pickle\"\n",
        "\n",
        "    with open(config_output_filename, 'wb') as config_f:\n",
        "      pickle.dump(C,config_f)\n",
        "      print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))\n",
        "\n",
        "    random.shuffle(all_imgs)\n",
        "\n",
        "    num_imgs = len(all_imgs)\n",
        "\n",
        "    # split to train and val\n",
        "    train_imgs = [s for s in all_imgs if s['imageset'] == 'trainval']\n",
        "    val_imgs = [s for s in all_imgs if s['imageset'] == 'test']\n",
        "\n",
        "    print('Num train samples {}'.format(len(train_imgs)))\n",
        "    print('Num val samples {}'.format(len(val_imgs)))\n",
        "\n",
        "\n",
        "    data_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train')\n",
        "    data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode='val')\n",
        "\n",
        "    # set input shape\n",
        "    input_shape_img = (None, None, 3)\n",
        "\n",
        "    img_input = Input(shape=input_shape_img)\n",
        "    roi_input = Input(shape=(None, 4))\n",
        "\n",
        "    # create rpn model here\n",
        "    # define the base network (resnet here, can be VGG, Inception, etc)\n",
        "    shared_layers = nn.nn_base(img_input, trainable=True)\n",
        "\n",
        "    # define the RPN, built on the base layers\n",
        "    # rpn outputs regression and cls\n",
        "    num_anchors = num_anchor_box_scales * num_anchor_box_ratios\n",
        "    rpn = nn.rpn(shared_layers, num_anchors)\n",
        "\n",
        "    model_rpn = Model(img_input, rpn[:2])\n",
        "\n",
        "    #load weights from pretrain\n",
        "    try:\n",
        "      print('loading weights from {}'.format(C.base_net_weights))\n",
        "      model_rpn.load_weights(C.base_net_weights, by_name=True)\n",
        "    #\tmodel_classifier.load_weights(C.base_net_weights, by_name=True)\n",
        "      print(\"loaded weights!\")\n",
        "    except:\n",
        "      print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n",
        "        https://github.com/fchollet/keras/tree/master/keras/applications')\n",
        "\n",
        "    # compile model\n",
        "    optimizer = Adam(lr=1e-5, clipnorm=0.001)\n",
        "    model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\n",
        "    model_rpn.summary()\n",
        "\n",
        "    # write training misc here\n",
        "    epoch_length = 100\n",
        "    num_epochs = 50\n",
        "    iter_num = 0\n",
        "\n",
        "    losses = np.zeros((epoch_length, 5))\n",
        "    rpn_accuracy_rpn_monitor = []\n",
        "    rpn_accuracy_for_epoch = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    best_loss = np.Inf\n",
        "\n",
        "    class_mapping_inv = {v: k for k, v in class_mapping.items()}\n",
        "    print('Starting training')\n",
        "\n",
        "    vis = True\n",
        "\n",
        "    # start acutual training here\n",
        "    #X, Y, img_data = next(data_gen_train)\n",
        "    #\n",
        "    ##loss_rpn = model_rpn.train_on_batch(X, Y)\n",
        "    #P_rpn = model_rpn.predict_on_batch(X)\n",
        "\n",
        "    # you should enable NMS when you visualize your results.\n",
        "    # NMS will filter out redundant predictions rpn gives, and will only leave the \"best\" predictions.\n",
        "    # P_rpn = model_rpn.predict_on_batch(image)\n",
        "    # R = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
        "    # X2, Y1, Y2, IouS = roi_helpers.calc_iou(R, img_data, C, class_mapping)\n",
        "    # this will output the binding box axis. [x1,x2,y1,y2].\n",
        "\n",
        "    Callbacks=keras.callbacks.ModelCheckpoint(\"./models/rpn/rpn.\"+C.network+\".weights.{epoch:02d}-{loss:.2f}.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=4)\n",
        "    callback=[Callbacks]\n",
        "    if len(val_imgs) == 0:\n",
        "        # assuming you don't have validation data\n",
        "        history = model_rpn.fit_generator(data_gen_train,\n",
        "                        epochs=num_epochs, steps_per_epoch = 1000, callbacks=callback)\n",
        "        loss_history = history.history[\"loss\"]\n",
        "    else:\n",
        "        history = model_rpn.fit_generator(data_gen_train,\n",
        "                        epochs=num_epochs, validation_data=data_gen_val,\n",
        "                        steps_per_epoch=1000, callbacks=callback, validation_steps=100)\n",
        "        loss_history = history.history[\"val_loss\"]\n",
        "\n",
        "    import numpy\n",
        "    numpy_loss_history = numpy.array(loss_history)\n",
        "    numpy.savetxt(C.network+\"_rpn_loss_history.txt\", numpy_loss_history, delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opDtnQqSmlj_",
        "colab_type": "text"
      },
      "source": [
        "## 2.Text Refinement Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNbTSU-Jmqhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}